# -*- coding: utf-8 -*-
"""python me781project attempt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BeVbfDNKJoWSNEkPbfwgt36jlmOIeTyv

### Co-authored by Tanya, Bhavik, Mudit, Srihit and Jaykumar as a result of our ME781 Data Mining final project.

Based on Data set from UCI's Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import pickle

"""## Reading Data"""

# Reading shopping data
X_train = pd.read_csv('ShoppingData.csv')
df = X_train.copy()

"""## Producing dummy variables for categorical data and cleaning data"""

dummiesdf = pd.get_dummies(df['VisitorType'])
df.drop('VisitorType', inplace = True, axis = 1)
df['New_Visitor'] = dummiesdf['New_Visitor']
df['Other'] = dummiesdf['Other']
df['Returning_Visitor'] = dummiesdf['Returning_Visitor']

dfmonth = pd.get_dummies(df['Month'])
df.drop('Month', inplace = True, axis = 1)
dfwithdummies = pd.concat([df, dfmonth], axis = 1, sort = False)

dfwithdummies['Class'] = df['Revenue'].astype(int)
dfwithdummies.drop('Revenue', axis = 1, inplace = True)
dfwithdummies['Weekend'] = df['Weekend'].astype(int)
dfwithdummies.drop('Returning_Visitor', axis = 1, inplace = True)
dfcleaned = dfwithdummies.copy()

X = dfcleaned.drop('Class', axis = 1)
Y = dfcleaned['Class'].copy()

"""## Checking for Collinearity Between Features and Creating Reducing Feature Size"""

cor = X.corr()

#sns.heatmap(cor, xticklabels=cor.columns,yticklabels=cor.columns)

# dfcleaned[dfcleaned['New_Visitor'] == 1]['Administrative'].value_counts()
def AvgMinutes(Count, Duration):
    if Duration == 0:
        output = 0
    elif Duration != 0:
        output = float(Duration)/float(Count)
    return output

Columns = [['Administrative', 'Administrative_Duration'], ['Informational', 'Informational_Duration'], ['ProductRelated', 'ProductRelated_Duration']]


X['AvgAdministrative'] = X.apply(lambda x: AvgMinutes(Count = x['Administrative'], Duration = x['Administrative_Duration']), axis = 1)
X['AvgInformational'] = X.apply(lambda x: AvgMinutes(Count = x['Informational'], Duration = x['Informational_Duration']), axis = 1)
X['AvgProductRelated'] = X.apply(lambda x: AvgMinutes(Count = x['ProductRelated'], Duration = x['ProductRelated_Duration']), axis = 1)
X.drop(['Administrative', 'Administrative_Duration','Informational', 'Informational_Duration','ProductRelated', 'ProductRelated_Duration'], axis = 1, inplace = True)

cor = X.corr()
#sns.heatmap(cor, xticklabels=cor.columns,yticklabels=cor.columns)

# Scaling to normalize data
X_copy = X.copy()
rc = RobustScaler()
X_rc=rc.fit_transform(X_copy)
X_rc=pd.DataFrame(X_rc,columns=X.columns)

"""## Finding Important Features then Removing from Dataframe"""

list_one =[]
feature_ranking = SelectKBest(chi2, k=5)
fit = feature_ranking.fit(X, Y)

fmt = '%-8s%-20s%s'

for i, (score, feature) in enumerate(zip(feature_ranking.scores_, X.columns)):
    list_one.append((score, feature))

dfObj = pd.DataFrame(list_one)
#dfObj.sort_values(by=[0], ascending = False)

X_rc=X_rc[["PageValues", "AvgInformational", "AvgAdministrative", "AvgProductRelated", "New_Visitor", "SpecialDay", "BounceRates", "ExitRates"]]

#X_rc.drop(['Aug','TrafficType','OperatingSystems','Other','Jul'],axis=1,inplace=True)

# Splitting the Dataset
X_train1, X_test1, y_train1, y_test1 = train_test_split(X_rc,Y,test_size=.2)

# Instantiating RandomForestClassifier() Model
clf1 = RandomForestClassifier(n_estimators= 200, max_depth = 30 )

# Training/Fitting the Model
clf1.fit(X_train1, y_train1)

pickle.dump(clf1, open('iri.pkl', 'wb'))

#accuracy_score(y_test1, y_pred)
